# PC 端 2D 照片级数字人引擎落地清单（LivePortrait + 自研 Audio2Motion 路线）

## 1. 需求与总体目标

- 需求场景
  - 运行平台：PC 端（无需适配手机端，算力可相对宽松）。
  - 形态：2D 照片级头像（单张人像驱动说话与轻微表情/头动）。
  - 交互：支持实时/准实时对话（接入 TTS/ASR/LLM 等上层模块）。
- 性能目标
  - 帧率：≥ 25 FPS（渲染侧）。
  - 总延迟：语音发出到嘴型开始动作 ≲ 100–200ms。
  - 分辨率：核心模型在 256×256 上工作，可选放大输出。
- 技术目标
  - 完全自控：模型结构、资源格式、接口协议可由我们定义与演进。
  - 可替换：渲染内核与 Audio2Motion 可独立替换升级。

总体技术路线：**LivePortrait 作为渲染内核（Motion→Image） + 自研 Audio2Motion（Audio→Motion） + 自定义 Avatar 格式 + 实时调度框架**。

---

## 阶段 0：环境与硬件准备

- 硬件
  - 一张独显（建议 RTX 3060 及以上，显存 ≥ 8G）。
- 软件
  - 安装 CUDA + cuDNN。
  - 安装 Python（建议 3.10）+ PyTorch（GPU 版本）。
  - 安装 C++ 编译工具链（后续 ONNX/部署可能需要）。

---

## 阶段 1：跑通 LivePortrait 渲染内核（图像驱动）

1. 获取代码与模型
   - 从 GitHub clone LivePortrait（OpenGVLab）仓库。
   - 按 README 安装依赖、下载官方权重。
2. 运行官方 demo
   - 使用官方提供的 source 图 + 驱动视频，生成离线视频验证效果。
   - 在 256×256 分辨率下测单帧前向耗时，估算可达帧率（目标 ≥ 20–30FPS）。
3. 抽象为“逐帧渲染器”
   - 在 LivePortrait 项目中找出：
     - 模型初始化部分（加载权重、放入 GPU）。
     - 单帧推理部分（输入 source + driving 表示 → 输出一帧图像）。
   - 封装为：
     - `class LivePortraitRenderer:`
       - `__init__(avatar_root_path)`
       - `render_frame(driving_code) -> frame_rgba`

> 阶段 1 目标：能手动构造一串 driving_code（例如从驱动视频提取），逐帧调用 `render_frame`，在窗口中实时显示（OpenCV/Qt 等），FPS ≥ 20。

---

## 阶段 2：设计并固化 Avatar 资源格式

1. 设计目录结构（自定义标准）

```text
avatar_root/
  avatar.json        # 配置：分辨率、模型版本等
  source.png         # 对齐好的人脸图（训练分辨率）
  mask.png           # 可选：前景/背景分割
```

2. `avatar.json` 建议字段
   - `resolution`: 如 `256` 或 `256x256`。
   - `renderer_model`: 如 `"liveportrait_v1"`。
   - 其他：背景色、是否开启头动、眨眼策略等。

3. 在 `LivePortraitRenderer.__init__` 中：
   - 从 `avatar_root` 读取 `avatar.json`、`source.png` 等。
   - 执行必要的预处理（缩放、裁剪、对齐）。

> 阶段 2 目标：用不同的 `avatar_root` 即可加载不同数字人，初步完成“Avatar 包”的抽象。

---

## 阶段 3：实时渲染主循环（先用“视频驱动”）

1. 搭建实时渲染 demo（Python）
   - 创建渲染窗口（OpenCV `imshow`、PyQt、Pygame、SDL 均可）。
   - 启动渲染线程（或主循环）：
     - 每隔 40ms（25FPS）：
       - 从驱动队列取一个 `driving_code`。
       - 调用 `LivePortraitRenderer.render_frame(driving_code)` 获取一帧图像。
       - 显示到窗口。

2. 驱动信号来源（先不接音频）
   - 调研 LivePortrait 推理流程，在驱动视频前向时：
     - 将每一帧对应的 internal driving 表示（关键点、motion code 等）dump 到文件。
   - 在实时 demo 中：
     - 预先加载这串 driving_code 到内存。
     - 按时间顺序循环送入渲染队列。

> 阶段 3 目标：不考虑音频驱动，只要有一段已知 driving_code，就能持续实时渲染并显示，FPS、延迟情况清晰可见。

---

## 阶段 4：Audio2Motion 原型（音频 → Motion）

### 4.1 定义“Motion 表达”格式

这是引擎内部的关键接口，推荐：

- 方案 A：使用 LivePortrait 内部的 motion 表达（如关键点 + 姿态），按其维度和编码方式建模。
- 方案 B：自己定义：
  - 从人脸视频提取 2D 关键点（嘴、下巴、眉毛为主）。
  - 再附加头部姿态 `yaw/pitch/roll`。
  - 将这些拼成一个固定维度向量 `motion_vec`。

> 之后所有模型以此 `motion_vec` 作为标准输出，这就是你的“Motion 接口格式”。

### 4.2 数据集与标注

1. 数据来源
   - 使用公开说话人脸数据集（如 VoxCeleb 家族等）。
2. 标注流程
   - 对每个视频：
     - 提取同步音频（重采样为 16kHz PCM/WAV）。
     - 对每帧视频：
       - 调用 LivePortrait 的关键点/运动估计模块，或其他人脸关键点/3DMM 模型。
       - 转换为你的 `motion_vec`（例如归一化关键点 + 姿态）。
   - 得到对齐数据：
     - `<音频帧序列, motion_vec 序列>`。

### 4.3 音频特征提取

- 训练阶段：
  - 将音频按 40ms 帧长、10ms 步长切帧。
  - 提取 mel-spectrogram 或 HuBERT/Wav2Vec2 embedding。
- 对齐规则：
  - 保证音频特征时间步与 `motion_vec` 帧对齐（同一时间点）。

### 4.4 轻量 Audio2Motion 模型设计

- 输入：过去 T 帧音频特征序列（例如 0.6s 窗口，对应 ~15 帧）。
- 输出：当前时间点的 `motion_vec`。
- 模型结构建议：
  - 例如：
    - 2–3 层 TCN（1D 卷积 + dilation）或 Bi-LSTM/小型 Transformer。
    - 顶部接 1–2 层全连接映射到 `motion_vec` 维度。
- 损失函数：
  - 主损失：L1/MSE（预测 `motion_vec` vs 标注 `motion_vec`）。
  - 正则：时间平滑 loss（相邻帧 `motion_vec` 差分的 L2 惩罚），减少抖动。

### 4.5 实时推理包装

- 封装类 `Audio2Motion`：
  - `push_audio(pcm_chunk)`：
    - 接收短 PCM 片段（例如 20–40ms）。
    - 内部更新音频特征缓冲区。
  - `pop_motion() -> motion_vec or None`：
    - 每隔 40ms 被调用一次，输出当前时间点的 `motion_vec`。
    - 可允许 100–200ms 的 look-ahead 以换嘴型准确度。

> 阶段 4 目标：给一段音频流，能连续输出与之匹配的嘴型/表情 `motion_vec` 序列，结构上已可对接 LivePortrait 渲染器。

---

## 阶段 5：端到端实时闭环（Audio2Motion + LivePortrait）

### 5.1 架构梳理

- 线程 A：音频输入
  - 从麦克风或 TTS 接收 PCM 流。
  - 持续调用 `Audio2Motion.push_audio(pcm_chunk)`。
- 线程 B：渲染
  - 每隔 40ms 调用 `Audio2Motion.pop_motion()` 获取最新 `motion_vec`。
  - 将 `motion_vec` 转换为 LivePortrait 所需的 `driving_code`。
  - 调用 `LivePortraitRenderer.render_frame(driving_code)`，显示图像。
- 两线程通过共享 `motion_vec` 缓冲区或队列用时间戳简易同步。

### 5.2 实时策略

- 固定帧率：25FPS（40ms 一帧）。
- 总延迟控制：
  - Audio2Motion 允许 100–200ms look-ahead（模型使用未来少量音频帧），换取嘴型准确度与稳定性。
  - 渲染前延迟不再增加，只是固定节奏输出。
- 回退策略：
  - 若某帧暂时拿不到新的 `motion_vec`，使用上一帧 `motion_vec` 以避免卡顿。

### 5.3 调参与体验

- 使用 TTS 生成可重复的测试语音，便于对比效果。
- 优先调这些方面：
  - 嘴型准确度：音节与开闭口时机是否匹配。
  - 平滑度：嘴型是否抖动、是否有“抽动”现象。
  - 头动/眨眼：初版可先固定，后续再用额外模型/规则驱动。

> 阶段 5 目标：获得一个真实“听音说话”的实时 2D 数字人 demo，尽管模型未完全极致优化，但端到端闭环已经跑通。

---

## 阶段 6：工程化与加速（ONNX / C++ SDK）

### 6.1 模型导出与推理引擎

- 将 Audio2Motion 与 LivePortrait Renderer 的 PyTorch 模型导出为 ONNX：
  - 固定输入尺寸、batch=1。
  - 尽量使用静态 shape，减少动态图开销。
- 在 C++ 中使用 ONNXRuntime 或 TensorRT：
  - 分别测试单帧推理耗时。
  - 启用 FP16 / TensorRT 优化（视显卡支持情况）。

### 6.2 C++/Rust SDK 封装

- 目标封装一个核心动态库，例如 `libavatar_engine.so`，暴露接口：

```c
int init_avatar_engine(const char* avatar_root);
int start_session();
int push_audio(const float* pcm, int length);
int pull_frame(uint8_t* rgba_buffer, int width, int height); // 或回调方式
int stop_session();
```

- 上层语言（Python/Go/Node/Unity 等）只需绑定这些 C 接口即可使用。

### 6.3 性能与质量优化

- 分辨率策略：
  - 训练与主推理在 256×256。
  - 需要 720p/1080p 时先简单上采样，再根据需求引入轻量超分模块。
- 模型剪枝与蒸馏：
  - 减少通道数、深度。
  - 用大模型 teacher 指导小模型训练（减少质量损失）。
- 多线程与异步：
  - 音频处理与渲染线程分离。
  - 预先计算下一帧（或小批帧）的 motion/图像，减少抖动。

> 阶段 6 目标：形成可集成到其他项目中的“数字人引擎 SDK”，外部只看到稳定的 API 与资源格式，而内部模型和实现完全由你掌控。

---

## 当前实施进度与已完成工作（仓库内）

### 已完成的代码与结构

- 工程结构与基础说明
  - 在 `engine/` 下搭建原型工程结构，并编写简要说明（`engine/README.md`）。
- Avatar 资源与配置
  - `engine/avatar_loader.py`：加载 `avatar.json` 与源图像，统一缩放到目标分辨率。
  - 示例 Avatar 包：`engine/avatars/demo/`，使用 Duix 的 Leo 头像作为源图。
- Motion 表达与 Audio2Motion 接口（占位版）
  - `engine/motion.py`：
    - 定义 `MotionVector`（当前为 6 维）：`jaw_open, mouth_wide, mouth_narrow, head_yaw, head_pitch, head_roll`。
    - 提供 `dim()`、`zeros()` 与 `clamp_motion()` 等工具函数。
  - `engine/audio2motion_stub.py`：
    - `Audio2MotionStub`：按固定帧率输出简单的 `MotionVector` 序列。
    - 当前实现基于多路正弦函数生成嘴型开合与轻微头动，用于代替真实音频模型进行端到端联调。
- 实时渲染 demo（占位 + LivePortrait 集成）
  - `engine/demo_realtime_stub.py`：
    - 从 Avatar 包加载源图。
    - 循环调用 `Audio2MotionStub.pop_motion()` 得到 `MotionVector`。
    - 调用 `LivePortraitRenderer.render_frame(motion)` 获取图像并显示，形成持续实时渲染。

### LivePortrait 的接入情况

- 已克隆 LivePortrait 源码
  - 仓库位置：`engine/third_party/LivePortrait`（KlingTeam/LivePortrait）。
  - 内含官方的 `inference.py`、`src/live_portrait_pipeline.py`、`src/live_portrait_wrapper.py` 等代码（权重需按其 README 自行下载到 `pretrained_weights`）。
- LivePortrait 后端封装（单帧推理）
  - 文件：`engine/liveportrait_backend.py`：
    - `LivePortraitBackend` 与 `LivePortraitConfig`：
      - 将 `engine/third_party/LivePortrait` 加入 `sys.path`，导入 `src.config.inference_config.InferenceConfig` 与 `src.live_portrait_wrapper.LivePortraitWrapper`。
      - 使用默认的 `InferenceConfig()` 初始化人类模型；关闭 crop/pasteback/旋转，只接受 256×256 的已裁剪头像。
    - 在首次渲染时：
      - 对源图做一次预处理与前向推理，得到：
        - 标准化输入张量 `x_s_tensor`（RGB）；  
        - `source_kp_info`（包含 `kp/exp/t/scale/pitch/yaw/roll`）；  
        - canonical keypoints `x_s = transform_keypoint(source_kp_info)`；  
        - 特征体 `f_s = extract_feature_3d(x_s_tensor)`；  
      - 将 `f_s`、`x_s` 与 `source_kp_info` 缓存，用于后续每帧驱动。
    - 在每帧渲染时：
      - 若提供 `MotionVector`：
        - 从中读取 `jaw_open, head_yaw, head_pitch, head_roll`。
        - 在缓存的 `source_kp_info` 上叠加姿态增量：
          - 将 `[-1,1]` 的 yaw/pitch/roll 映射为约 ±20°/15°/10° 的角度增量。
          - 更新 `kp_info['yaw'/'pitch'/'roll']` 后，再调用 `transform_keypoint(kp_info)` 得到新的 driving keypoints `x_d`。
        - 基于 `jaw_open` 对 `x_d` 的下半部分关键点做轻微向下平移，形成简化嘴型开合效果。
      - 若不提供 `MotionVector`，则回退为 `x_d = x_s`（自驱动）。
      - 调用 `warp_decode(f_s, x_s, x_d)` + `parse_output(...)` 得到一帧 RGB 图像，再转换为 BGR 输出。
- 渲染器对接后端
  - `engine/renderer.py`：
    - `LivePortraitRenderer` 在初始化时：
      - 自动检查 `engine/third_party/LivePortrait` 是否存在；若存在，则构造 `LivePortraitBackend`。
      - 若后端初始化成功，`render_frame()` 会优先走 LivePortrait 模型；否则回退为原来的“直接返回源图像”占位模式。

### 已完成的小结（当前状态）

- 需求侧：
  - 已明确：PC 端 2D 照片级头像、实时、可与上层对话系统衔接、完全自控模型与格式。
- 计划侧：
  - 已细化 0–6 阶段路线，并落实到实际目录结构与模块划分。
- 实现侧：
  - 已完成：
    - Avatar 包格式与加载。
    - MotionVector 定义与占位 Audio2Motion 接口。
    - LivePortrait 源码接入与单帧推理封装（人类模式）。
    - 将 MotionVector（嘴型 + 头姿）映射到 LivePortrait 的关键点空间的初版逻辑。
    - 搭建实时 demo，将上述模块串成“Motion → 图像”的实时渲染链路（目前由占位 Audio2MotionStub 驱动）。

---

## 下一步计划（短期）

1. **完善 MotionVector → LivePortrait 映射**
   - 细化嘴型：不再只用“下半关键点整体下移”，而是结合 LivePortrait 内置的 lip retargeting 模块（`retarget_lip`）和 lip_close_ratio，更精确控制嘴部开合与形状（宽/窄）。  
   - 头动与表情：继续调参 yaw/pitch/roll 的映射系数与范围，使头动自然且不过度摇晃，避免眩晕感。

2. **从占位 Audio2MotionStub 过渡到“轻量音频驱动”**
   - 在现有接口基础上新增一个简单的 Audio2Motion 实现：
     - 基于音频能量/包络驱动 `jaw_open`，初步让嘴型跟“响度”走。  
     - 后续再扩展为基于 mel 频谱或预训练语音模型 embedding 的轻量模型。
   - 保留 `Audio2MotionStub` 作为无音频时的测试模式。

3. **LivePortrait 性能与稳定性验证**
   - 在目标 PC 环境（指定 GPU 型号）上：
     - 实测单帧推理耗时（256×256 分辨率），确认可达到 ≥25FPS。  
     - 检查内存/显存使用情况，必要时关闭 `torch.compile`、调整半精度等参数。

4. **对接真实音频流**
   - 用简单的麦克风采集或已有 TTS 输出替换 demo 中的“伪 PCM”：  
     - 完成“真实音频流 → Audio2Motion（简版） → LivePortrait → 实时窗口”的闭环。  
   - 同步更新文档与示例代码，说明如何在应用中调用这条 pipeline。

5. **规划后续 C++/ONNXRuntime SDK 化工作**
   - 评估：将当前 Python 版 LivePortrait + Audio2Motion 转成 ONNX 的可行性与收益。  
   - 初步设计 C++ SDK 接口（`init_avatar / push_audio / pull_frame`），为后续工程化预留空间。

---

## 主要问题与风险评估

1. **MotionVector 设计过于简化**
   - 风险：当前仅有 6 维参数（`jaw_open, mouth_wide, mouth_narrow, head_yaw, head_pitch, head_roll`）：
     - 无法覆盖眼睑、眉毛、面颊等细节表情；
     - 难以充分利用 LivePortrait 在表情空间上的表达能力。
   - 影响：数字人容易出现“只有嘴和头在动，整体表情单一、僵硬”的问题。
   - 应对策略：
     - 外部接口层：保留 MotionVector 作为相对稳定的“抽象控制层”（便于 Audio2Motion 设计和跨引擎复用）；  
     - 内部实现层：在 LivePortrait 后端中，逐步扩展为以其内部 motion 表示为实际驱动接口，例如：
       - 直接映射到 `kp/exp/t/scale/pitch/yaw/roll` 的子集；
       - 或围绕 lip/eye ratio、局部关键点偏移构建更高维度的内部控制向量。

2. **ONNX 导出可行性未验证**
   - 风险：
     - LivePortrait 内部结构复杂，包含多模块、动态 shape 和潜在的自定义算子，整体导出为 ONNX/TensorRT 存在较高不确定性。  
   - 影响：
     - 若在阶段 6 才尝试整体 ONNX 化，可能遇到架构性障碍，导致大量重构或被迫放弃加速路径。
   - 应对策略：
     - 短期优先将“自研 Audio2Motion 模型”设计为易于 ONNX 导出的结构，并尽早验证导出与运行（阶段 3–4 即开始尝试）；  
     - LivePortrait 主体在中短期内保持 PyTorch 运行，仅对其中耗时明显且算子简单的子模块做局部 ONNX 化试验；  
     - 在文档和代码中预留“PyTorch 直跑 / ONNX 子模块”两条路径，避免过早绑定到某一加速方案。

3. **缺少自然动作（眨眼、微表情）**
   - 风险：
     - 如果数字人长时间不眨眼、不做细微表情，即使嘴型准确也会呈现“死脸”效果，严重影响沉浸感。  
   - 影响：
     - 用户会本能感到不适或“怪异”，降低对整体系统的认可度。
   - 应对策略：
     - 短期：在现有 MotionVector 上增加基于规则的自然动作：
       - 随机 2–5 秒触发一次眨眼（通过修改局部关键点或使用 LivePortrait 的眼睛 retargeting 通道实现）；  
       - 加少量低幅度、低频率的头部/眉毛轻微抖动。  
     - 中期：设计眼睛/眉毛/面颊等更细粒度的表情维度，纳入 MotionVector 内部表示，并在 LivePortrait 后端做对应映射。

4. **音视频同步机制不够精确**
   - 风险：
     - 若仅依靠“共享缓冲区 + 简单时间戳”，长时间运行后容易产生音画 drift（嘴型明显滞后或超前）。  
   - 影响：
     - 音画不同步是实时数字人的核心“致命伤”，会让用户非常敏感地察觉“假”的感觉。
   - 应对策略：
     - 以音频为主时钟：每个音频 chunk 在 Audio2Motion 阶段产生带精确 PTS 的 Motion；  
     - 渲染侧以统一时基选择最接近的 Motion 帧，而非简单“按顺序 pop”；  
     - 建立漂移检测机制：
       - 定期计算“当前音频时间 - 当前渲染时间”的差值；  
       - 超过阈值（如 50–80ms）时，通过跳帧、重复帧或轻微拉伸节奏进行校正；  
     - 后续可在文档中明确对齐策略（如“音频为主时钟，渲染对齐最近 Motion 帧”），便于跨模块协作。

## 总结

- 使用 **LivePortrait** 作为高质量、可实时的 2D 渲染内核。
- 自研 **Audio2Motion**，负责音频→嘴型/表情/姿态参数的流式预测。
- 通过自定义的 **Avatar 包格式 + Motion 接口格式 + SDK 封装**，
  搭建起一套可长期演进、模型和格式完全自控的 PC 端 2D 照片级数字人底层引擎。
